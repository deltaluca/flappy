\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{cite}

\title{AI player for diplomacy game}
\author{Matthias Hueser}
\date{today}

\begin{document}
\maketitle

\section{Challenges for AI player}

Diplomacy is generally seen as an interesting sandbox to explore
different AI techniques for multiple-player strategy games.

It can be classified as a multi-player, cooperative, zero-sum game
with imperfect information, all of which have important ramifications
for the development of an AI.

Each player has imperfect information because the game state is not
changed in reaction to each players movement separately. Instead a 
so-called 'move-resolution' phase takes place. Here players formulate
their moves in secret and then submit them simultaneously to the game
server which resolves any conflicts, 'standoffs'
in Diplomacy parlance. Being unaware of the others players actions means
that the immediate effects of its own movement orders are uncertain. The
game mechanics dictate that at the end of a turn a chain of so-called 
stand-offs might occur, in case several move order conflict. As a result
an AI player is uncertain of the effect of its own actions. Another matter
that increase uncertainty are support orders that bias the resolution of
stand-offs in favor of a particular player. 

Diplomacy is cooperative since negotiation is at the heart of the game:
It allows players to communicate their short- / long-term intentions before
their actual moves are revealed. For instance in stand-off situations it will
enable two players to attack a field of a third-party. In general negotation
will decrease uncertainty but it can be a double-edged sword. Players might
pretend their actual intentions but actually conspire to betray each other. Hence
in essence an AI player will need to establish whom to trust when considering
his options based on the messages of other players.


\section{Iterative AI development}

An iterative approach is envisioned where generic techniques
like 'Game tree search' are implemented first. The reason for this is that 
search is generally considered as the easiest to implement. Apart from
a utility function mapping a game state to a certain metric vector no 
domain knowledge of the game is required. It is considered a primitive
approach but is required to create a foundation for further development.

As discussed in the next section powerful AI play requires learning of
techniques and responses to typical game situations. The 'partial-state
database' outlined in the next section will form the second iteration
of the AI. 

Throughout the implementation of the two components 'search' and 'knowledge
database' emphasis is laid on generality which enables a close integration
with the 'negotation module', which enables intelligent forming of 
alliances and communication of intentions to other players.

\section{Short-term game tactics}

For short-term game tactics a search with minimal look-ahead can be used.

\subsection{Searching the game tree}

Searching a game tree is considered a generic technique for playing 
games that have discrete states connected by edges which represent movement
orders. As outlined in the introductory section there is uncertainty about
the effects of movement orders, making the viability of this approach doubtable. 

Besides at any given state of the game there is large number of moves possible
for each player, giving rise to a large branching factor in the game tree. [SHA08]
have estimated the number of a possible moves in a game situation to be on the order
of 2_{91} to get a feeling for the numbers involved. Even moderate look-ahead in
the search space almost certainly puts a strain on the computational resources.
To overcome this difficulty, search heuristics for pruning the game tree and scheduling
the exploration order of the game branches are alternative-less extension of a
brute-force search. Both techniques can be implemented through so-called utility function
defined on a game state. We discuss the structure of utility functions for Diplomacy
in a later section.

As [SHA08] have argued, modelling the game with payoff matrices is similarly mis-guided
as a brute-force game tree search and so we have not considered it for our AI player.

\subsubsection{Search heuristics}

Whenver a brute-force search is not possible we need to assign a utility to an 
intermediate game state, in which the winner has not yet been determined. On the 
one hand this enables us to cut-off a game-tree search at a shallow level and
still propagate move values upwards through a MinMax procedure. On the other hand
they enable us to guide the MinMax by scheduling the exploration of branches or
pruning branches with limited attractiveness altogether. Even generic heuristics
like AlphaBeta-Pruning or NegaScout that do not use any knowledge of the game give
speed improvements of several orders of magnitude [REFERENCE]. Hence we expect
informed searches to give us considerable latitude to search deeper. 

\subsubsection{State evaluation function}

The aforementioned state utility function will need to incorporate knowledge
of the game to be effective. As the objective of the game is to control the 
majority of the supply-centers any utility function should consider how likely
it is to gain additional centers from a given game state. [Loeb92] propose to 
weight the control centers controlled by a player. Supply centers that are
prone to be attacked in the next turns have a smaller weight whereas ones with
a solid defence base have high weight. All of these functions are blind to 
alliances / cooperations between players in the sense that they treat every 
player as an adversary. A better heuristic would consider the positions of
all allied players collectively, given that there exists some trust bond
between the allied players. 


\section{Long-term strategy templates}

\subsection{Strategy acquisation}

Several choices: 

a) Hard-coding of offensive / defensive 
   strategies that are played in certain
   situations

b) Start out with rudimentary strategies that
   are extended through learning from previous games.
   (see section 'Learning on previous game-databases')

\subsection{Expert game knowledge}

To improve the AIs playing skills over the lifetime, a
knowledge base with various parts needs to be constructed.

According to the current game state and other parameters, such
as the remaining time in the game or a model of the other players
the agent picks a response rule and generates from it a candidate
move. 

As [SHA08] propose, a game database contains partial-state -> response movement
pairs. A partial state is a high-level description of a state, omitting
details which are not relevant to the formulation of strategy / tactics. If
such a mapping is not found the current state is classified and a candidate move
is created using search techniques. Having found this move, a new entry in the 
database is created. At regular time-points in the future a payoff is calculated
(based on future game metrics) and associated with the new partial-state -> response pair.
The motivation behind this is to rank responses to typical game situations. This
approach reduces the computational load as search is a very costly operation in the
game of Diplomacy. Besides ranking newly-created response upon termination of a game
all used movements are re-ranked using a process called 'Temporal difference learning'.

\subsubsection{Partial game-state encoding}

A game state can be classifed by

a) The current position of all players units
b) A model of all other players (see later) in that situation 
b) Game parameters, including game phase
c) The history of the game


\subsubsection{Opening games}

The opening of games is a special case since a large number
of tournament opening books exist. Since these initial states are amenable to
analysis we can assume that master players had ample opportunities to
pick optimal strategies. Our AI agent has access to such a collection
and will pick a suitable move according to his denomination.

\subsection{Move generation}

Given a description of the game state the AI players classifies
it with respect to several properties and assigns to it a 'partial state'
which is taken from an evolving pattern database. For example in a cooperative
setting a coordinated attack on an opponent would classify as a distinct
'partial state'. As this shows the description should be formulated a level
above move descriptions, so that a chain of movement emerges.

\subsection{Offensive strategies}

\subsection{Defensive strategies}

\subsection{Measures of a player position}


\section{Negotiation component}

The sub-system of the AI responsible for negotation
is seen as virtually independent from the search techniques
and expert database discussed above. Nonetheless it is 
considered vital when competing against players which
use negotation. Consider the situation where offensive alliances
are created against the AI player. To overcome this aggression 
the AI player needs communication to organize defense and launch
retaliation attacks. The product of negotiation are commonly
collective strategies which guide the army movements of the
individual players. In order to safe-guard against other
players who propose reckless strategies those are always
cross-checked against the movements favoured by the search
and knowledge base components.

\subsection{Internal model of other players}

\subsubsection{Trust}

Trust as a relationship between two players removes contingency
from the 'move search process'. This is because certain actions such
as an attack on its own units can be ruled out if a substantial bond of trust
has been established. Besides consistent actions, communication
is needed to forge such relationships among players over time. 

\section{Game meta-heuristics}

\subsection{Learning on previous game-databases}

Learning through pattern-weights (see SHA08):

  Idea: Adapt generic initial strategy by performing 
        self-play, temporal difference method

Reinforcement learning:

  Idea: Represent the environment and use a stochastic
        transition model contingent on other players
        moves (could try to fill in gaps of knowledge 
        with a characterization of other players)

  

\subsection{Genetic -- adapting parameter vector}

\section{bonus, bonus: sentiment analysis of free-form player text}

\end{document}
